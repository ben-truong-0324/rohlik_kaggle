

import re
import time
import os
from datetime import datetime
import unittest
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tabulate import tabulate 
import pickle
import random
from copy import deepcopy
import math

from scipy.stats import ttest_1samp

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix,precision_score, \
                        recall_score,classification_report, \
                        accuracy_score, f1_score, log_loss, \
                       confusion_matrix, ConfusionMatrixDisplay,\
                          roc_auc_score, matthews_corrcoef, average_precision_score
from sklearn.cluster import KMeans, AgglomerativeClustering,DBSCAN,Birch,MeanShift, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.model_selection import ParameterSampler

#import dimension reduction modules
from sklearn.decomposition import PCA, FastICA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold


from torch import nn, optim
import torch
from torch.utils.data import DataLoader, TensorDataset


from sklearn.cluster import KMeans
import numpy as np

class MLPRegression(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim=2048, dropout_rate=0.1):
        super(MLPRegression, self).__init__()
        self.dropout = nn.Dropout(dropout_rate)
        
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)
        self.fc3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)
        self.fc4 = nn.Linear(hidden_dim // 4, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout(x)
        x = torch.relu(self.fc3(x))
        x = self.dropout(x)
        return self.fc4(x)  # Output a continuous value

class CNNRegression(nn.Module):
    def __init__(self, input_dim, output_dim, kernel_size, hidden_dim, dropout_rate):
        super(CNNRegression, self).__init__()
        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=kernel_size, padding=kernel_size//2)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.fc1 = nn.Linear(hidden_dim * (input_dim // 2), hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)  # Flatten the tensor for fully connected layers
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class LSTMRegression(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, dropout_rate):
        super(LSTMRegression, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_rate)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])  # Use the last output of the LSTM for regression
        return output


class SalienceNNRegression(nn.Module):
    def __init__(self, input_dim, output_dim,hidden_dim, dropout_rate,n_clusters=128):
        super(SalienceNNRegression, self).__init__()
        # K-Means clustering model to find clusters
        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        # MLP layer to process the clustered data
        self.fc1 = nn.Linear(input_dim *2, hidden_dim) 
        self.fc2 = nn.Linear(hidden_dim, output_dim) 
        self.relu = nn.ReLU()

    def fit_kmeans(self, X_train):
        self.kmeans.fit(X_train)
    

    def forward(self, X):
        if not hasattr(self.kmeans, "cluster_centers_"):
            raise RuntimeError("KMeans model is not fitted. Please call `fit_kmeans` with training data before using the model.")
        # Step 1: Get the cluster assignments for the input instances
        X_np = X.cpu().numpy().astype(float)
        cluster_labels = self.kmeans.predict(X_np)
        # Step 2: Get the cluster centroids (hidden states) and ensure correct dtype
        centroids = self.kmeans.cluster_centers_
        cluster_context = centroids[cluster_labels]  # Shape: (batch_size, input_dim)
        combined_input = torch.cat((X, torch.tensor(cluster_context).float().to(X.device)), dim=1)
        x = self.relu(self.fc1(combined_input))
        x = self.fc2(x)
        return x